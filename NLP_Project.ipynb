{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LN2nC0jv26-8"
      },
      "source": [
        "## Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5U-SBmQ26-9"
      },
      "source": [
        "### 1. Preliminary analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yevO4Pa626-9"
      },
      "source": [
        "| Task    | Assigned to | Status |\n",
        "|:----------------|:------------|:------:|\n",
        "| What type of documents does it contain?  | Alberto | ✅ |\n",
        "|       How many documents are there?       | Alberto | ✅ |\n",
        "|       Calculate and visualise some simple statistics for the collection, e.g. the average document length, the average vocabulary size, etc.       | Tutti | ❌ |\n",
        "|    BIO tagging for each file.       | Paolo | ✅ |\n",
        "| Create datasets with sentences from the tagged dataset. | Paolo | ✅ |\n",
        "| Create merged datasets | Paolo | ✅ |\n",
        "| Cluster the documents and visualise the clusters to see what types of groups are present | Paolo | ❌ |\n",
        "| Index the documents so that you can perform keyword search over them | Leonardo | ❌ |\n",
        "| Train a Word2Vec embedding on the data and investigate the properties of the resulting embedding | Alberto | ❌ |\n",
        "\n",
        "\n",
        "> **_KEY:_** [✅]() Completed [❌]() Not Completed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFmjHq0_26--"
      },
      "source": [
        "### 2. Training models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnshwzp726--"
      },
      "source": [
        "| Task    | Assigned to | Status |\n",
        "|:----------------|:------------|:------:|\n",
        "| Train a model to perform that task (by fine-tuning models on the training data)  | ??? | ❌ |\n",
        "| Test pre-trained models on the task (if they already exist)                      | ??? | ❌ |\n",
        "| Evaluate different models and compare their performance                          | ??? | ❌ |\n",
        "> **_KEY:_** [✅]() Completed [❌]() Not Completed\n",
        "\n",
        "> **_HINT_**: as a minimum here we would expect to see a linear classifier trained on the data (if an\n",
        "appropriate for the task) and compare it with deep learning model, such as BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ3Pspn926-_"
      },
      "source": [
        "### 3. Possible extensions:\n",
        "Depending on the dataset chosen there will be many additional investigations you can perform.\n",
        "For instance, oftentimes we can improve performance of a model on a particular task by simply\n",
        "including additional data that is related to the task in its training set. So see if you can find other\n",
        "data that helps with the task that you chose. Moreover, there are many NLP challenges out\n",
        "there, so if you can’t find more data for the task you’re working on, look for another interesting\n",
        "challenge to work on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqrFnKgS26-_"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!{sys.executable} -m spacy download it_core_news_sm;"
      ],
      "metadata": {
        "id": "b149DH7E3v-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WkdXO4h26-_"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import sys\n",
        "import spacy\n",
        "import it_core_news_sm\n",
        "import csv\n",
        "import os\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from tqdm import tqdm\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kp4QJsIObIsS"
      },
      "outputs": [],
      "source": [
        "RunningInCOLAB = 'google.colab' in str(get_ipython()) if hasattr(__builtins__,'__IPYTHON__') else False\n",
        "\n",
        "if RunningInCOLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    import os\n",
        "    os.chdir('/content/drive/MyDrive/Colab Notebooks/NLP/KIND-main')\n",
        "    os.getcwd()\n",
        "\n",
        "    print(\"Colab environment detected. Paths have been set.\")\n",
        "else:\n",
        "    print(\"You are not running in Google Colab. Skipping this step...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwRbIDK526_B"
      },
      "source": [
        "## Data Import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJxIzqDR26_B"
      },
      "source": [
        "In the following section we manipulate the dataset provided to get the full sentences that have been annotated by the original authors.\n",
        "\n",
        "First we create the directories if they do not exist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gidUc2qQ26_B"
      },
      "outputs": [],
      "source": [
        "os.mkdir(os.path.join(os.getcwd(), 'dataset')) if not os.path.exists(os.path.join(os.getcwd(), 'dataset')) else None\n",
        "os.mkdir(os.path.join(os.getcwd(), 'dataset/txt-version')) if not os.path.exists(os.path.join(os.getcwd(), 'dataset/txt-version')) else None\n",
        "os.mkdir(os.path.join(os.getcwd(), 'dataset/BIO-tagged-version')) if not os.path.exists(os.path.join(os.getcwd(), 'dataset/BIO-tagged-version')) else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_4rJimG26_C"
      },
      "source": [
        "### Merged dataset generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puOyycV926_C"
      },
      "source": [
        "We want to provide an additional dataset in which content of all the files is merged together. This will be useful for the subsequent tasks.\n",
        "\n",
        "First we merge the initial `.tsv` train files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sn8-n8gN26_C"
      },
      "outputs": [],
      "source": [
        "# Get the list of files in the dataset directory\n",
        "dataset_dir = os.path.join(os.getcwd(), 'dataset')\n",
        "dataset_files = os.listdir(dataset_dir)\n",
        "\n",
        "# Merging all train.tsv files together\n",
        "content = \"\"\n",
        "for file in tqdm(dataset_files):\n",
        "    # We are dealing with the train.tsv files only\n",
        "    if file.endswith('test.tsv') or not file.endswith('.tsv'):\n",
        "        continue\n",
        "\n",
        "    file_path = os.path.join(dataset_dir, file)  \n",
        "\n",
        "    # Read the content of the file and appending it to the content variable\n",
        "    with open(file_path, 'r') as f:\n",
        "        content += f.read()\n",
        "    \n",
        "# Write the merged content back to the file\n",
        "with open(f'{dataset_dir}/merged_dataset_train.tsv', 'w') as f:\n",
        "    f.write(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmiLMUg926_C"
      },
      "source": [
        "Now we do the same for the test files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CQEJbGV26_C"
      },
      "outputs": [],
      "source": [
        "# Get the list of files in the dataset directory\n",
        "dataset_dir = os.path.join(os.getcwd(), 'dataset')\n",
        "dataset_files = os.listdir(dataset_dir)\n",
        "\n",
        "# Merging all test.tsv files together\n",
        "content = \"\"\n",
        "for file in tqdm(dataset_files):\n",
        "    # We are dealing with the test.tsv files only\n",
        "    if file.endswith('train.tsv') or not file.endswith('.tsv'):\n",
        "        continue\n",
        "\n",
        "    file_path = os.path.join(dataset_dir, file)  \n",
        "\n",
        "    # Read the content of the file and appending it to the content variable\n",
        "    with open(file_path, 'r') as f:\n",
        "        content += f.read()\n",
        "    \n",
        "# Write the merged content back to the file\n",
        "with open(f'{dataset_dir}/merged_dataset_test.tsv', 'w') as f:\n",
        "    f.write(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W5AKXPJ26_D"
      },
      "source": [
        "### Complete sentences generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_JbGPPO26_D"
      },
      "source": [
        "Now for each train and test `.tsv` file we want to retrieve the compact form of the sentences that have been annotated by the original authors.\n",
        "\n",
        "For each file, sentences have been reconstructed and allocated one-for-row in the corresponding `.txt` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8-LShDP26_D"
      },
      "outputs": [],
      "source": [
        "# Get the list of files in the dataset directory\n",
        "dataset_dir = os.path.join(os.getcwd(), 'dataset')\n",
        "dataset_files = os.listdir(dataset_dir)\n",
        "\n",
        "# For each file in the dataset directory split the content when finding empty lines\n",
        "for file in tqdm(dataset_files):\n",
        "    # Do not read txt files\n",
        "    if not file.endswith('.tsv'):\n",
        "        continue\n",
        "\n",
        "    # Get the .tsv file path\n",
        "    file_path = os.path.join(dataset_dir, file)\n",
        "\n",
        "    # Inserting the content of the tsv file into a dataframe keeping the blank lines (i.e. the end of a sentence)\n",
        "    data_df = pd.read_csv(file_path, sep='\\t', names=['Word', 'Entity'], skip_blank_lines=False, quoting=csv.QUOTE_NONE)\n",
        "\n",
        "    # Replace NaN values with a new line (\\n) to mark the beginning of a new phrase\n",
        "    data_df.fillna('\\n', inplace=True)\n",
        "    \n",
        "    # Reconsetructing the sentences by joining the words together\n",
        "    sentences = \" \".join(data_df['Word']).replace('\\n ', '\\n') #.replace(' .', '.').replace(' ,', ',').replace(' !', '!').replace(' ?', '?').replace(' :', ':').replace(' ;', ';').replace(' %', '%').replace(' )', ')').replace('( ', '(').replace(' ]', ']').replace('[ ', '[').replace(' }', '}').replace('{ ', '{')\n",
        "    \n",
        "    # Write the content back to a text file\n",
        "    output_file_path = f\"{os.path.join(os.getcwd(), 'dataset/txt-version')}/{file[:-3] + 'txt'}\"\n",
        "    with open(output_file_path , 'w') as f:\n",
        "        f.write(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUSLjKFi26_D"
      },
      "source": [
        "### Bio tagging conversion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWtQd4bk26_D"
      },
      "source": [
        "Finally, we convert into BIO-tagging format the entities in the datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAvtxGNV26_E"
      },
      "outputs": [],
      "source": [
        "o_tag = \"O\"\n",
        "types = set()\n",
        "count = {}\n",
        "\n",
        "# Dictionary of input and output files names\n",
        "files = {\n",
        "\t\"wikinews_train.tsv\": \"WN_train.tsv\",\n",
        "\t\"wikinews_test.tsv\": \"WN_test.tsv\",\n",
        "\t\"fiction_train.tsv\": \"FIC_train.tsv\",\n",
        "\t\"fiction_test.tsv\": \"FIC_test.tsv\",\n",
        "\t\"degasperi_train.tsv\": \"ADG_train.tsv\",\n",
        "\t\"degasperi_test.tsv\": \"ADG_test.tsv\",\n",
        "\t\"moro_train.tsv\": \"AM_train.tsv\",\n",
        "\t\"moro_test.tsv\": \"AM_test.tsv\",\n",
        "\t\"merged_dataset_train.tsv\": \"MERGED_train.tsv\",\n",
        "\t\"merged_dataset_test.tsv\": \"MERGED_test.tsv\",\n",
        "}\n",
        "\n",
        "for file in tqdm(files):\n",
        "\twith open(f\"{os.path.join(os.getcwd(), 'dataset')}/{file}\", \"r\") as f:\n",
        "\t\t# Getting the output file name related to the current file\n",
        "\t\tout_file = files[file]\n",
        "\t\tcount[out_file] = {\"sentences\": 0, \"tags\": {}, \"tokens\": 0}\n",
        "\n",
        "\t\tsentences = []\n",
        "\t\tcurrent_sentence = []\n",
        "\n",
        "\t\tfor line in f:\n",
        "\t\t\tline = line.strip()\n",
        "\t\t\tif len(line) == 0:\n",
        "\t\t\t\tif len(current_sentence) > 0:\n",
        "\t\t\t\t\tsentences.append(current_sentence)\n",
        "\t\t\t\t\tcurrent_sentence = []\n",
        "\t\t\t\tcontinue\n",
        "\t\t\tparts = line.split(\"\\t\")\n",
        "\t\t\tcurrent_sentence.append(parts)\n",
        "\t\t\tcount[out_file][\"tokens\"] += 1\n",
        "\n",
        "\t\tif len(current_sentence) > 0:\n",
        "\t\t\tsentences.append(current_sentence)\n",
        "\n",
        "\t\tcount[out_file][\"sentences\"] = len(sentences)\n",
        "\n",
        "\t\t# BIO tagging conversion. The first non-O tag after a sequence of O-tags is converted to B-tag.\n",
        "\t\t# The following non-O tags (the ones that follow the B-tag) are converted to I-tags until\n",
        "\t\t# an O-tag is found. The same procedure is repeated for each sequence of non-O tags\n",
        "\t\tfor sentence in sentences:\n",
        "\t\t\tprevious_ner = o_tag\n",
        "\t\t\tfor token in sentence:\n",
        "\t\t\t\tner = token[1]\n",
        "\t\t\t\tnew_ner = ner\n",
        "\t\t\t\tif ner != o_tag:\n",
        "\t\t\t\t\tif previous_ner != ner:\n",
        "\t\t\t\t\t\tif ner not in count[out_file][\"tags\"]:\n",
        "\t\t\t\t\t\t\tcount[out_file][\"tags\"][ner] = 0\n",
        "\t\t\t\t\t\tnew_ner = \"B-\" + ner\n",
        "\t\t\t\t\t\tcount[out_file][\"tags\"][ner] += 1\n",
        "\t\t\t\t\t\ttypes.add(ner)\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tnew_ner = \"I-\" + ner\n",
        "\t\t\t\ttoken[1] = new_ner\n",
        "\t\t\t\tprevious_ner = ner\n",
        "\n",
        "\t\t# Writing the converted file into the appropriate directory\n",
        "\t\twith open(f\"{os.path.join(os.getcwd(), 'dataset/BIO-tagged-version')}/{out_file}\", \"w\") as fw:\n",
        "\t\t\tfor sentence in sentences:\n",
        "\t\t\t\tfor token in sentence:\n",
        "\t\t\t\t\tfw.write(token[0])\n",
        "\t\t\t\t\tfw.write(\"\\t\")\n",
        "\t\t\t\t\tfw.write(token[1])\n",
        "\t\t\t\t\tfw.write(\"\\n\")\n",
        "\t\t\t\tfw.write(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkmsZwe5slTW"
      },
      "source": [
        "## Data inspection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYBNtMxY26_E"
      },
      "source": [
        "KIND (Kessler Italian Named-entities Dataset) is an Italian dataset for Named-Entity Recognition (NER).\n",
        "\n",
        "The purpose of NER task is to tag all the named entity, namely identify all the objects in the real world.\n",
        "\n",
        "In this case there are three categories to annotate:\n",
        "- person (PER): a single individual, an animal or a group of humans with a proper name;\n",
        "- organization (ORG): every formally established association defined by an organizational structure;\n",
        "- location (LOC): geographical entities defined by political and/or social groups which possess a physical location and a proper name.\n",
        "\n",
        "The dataset is composed by four different collections with texts taken from: \n",
        "- Wikinews (WN) as a source of news texts, picking articles belonging to the last two decades; \n",
        "- Italian fiction books (FIC) in the public domain ; \n",
        "- writings and speeches from Italian politician Aldo Moro (AM);\n",
        "- public documents written by Alcide De Gasperi (ADG).\n",
        "\n",
        "The texts belong to three different domains: news, literature, and political discourses.\n",
        "\n",
        "The dataset contains more than one million tokens, of which around 600K are manually annotated instead the remaining part is semi-automatically annotated.\n",
        "\n",
        "| Dataset   | Documents |\n",
        "| --------- | --------- |\n",
        "| Wikinews  | 1,000 |\n",
        "| Fiction | 86 |\n",
        "| Aldo Moro | 250 |\n",
        "|Alcide De Gasperi | 158 |\n",
        "\n",
        "From the given files is not possible to distinguish the different documents but only the sentences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's analyze a single dataset, we take `degasperi_train.tsv"
      ],
      "metadata": {
        "id": "zR5S9-XJ9f-P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdNVsqC8cO-S"
      },
      "outputs": [],
      "source": [
        "# Path of file degasperi_train.tsv\n",
        "file_path = os.path.join(os.getcwd(), 'dataset/degasperi_train.tsv')\n",
        "\n",
        "# Each row of the dataframe is a word with associated type of entity\n",
        "data_df = pd.read_csv(file_path, sep='\\t', names=['Word', 'Entity'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wf3LKUHJN_9k"
      },
      "outputs": [],
      "source": [
        "# See first elements\n",
        "data_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FknmfAPu0WO7"
      },
      "outputs": [],
      "source": [
        "data_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All elements are not null."
      ],
      "metadata": {
        "id": "Ez4IwabkFj4u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZpMqqvP02YQ"
      },
      "outputs": [],
      "source": [
        "data_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gah00kAJ1MLx"
      },
      "outputs": [],
      "source": [
        "data_df[:25]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RK09MlSSgC-e"
      },
      "outputs": [],
      "source": [
        "counts_entity = Counter(data_df['Entity'])\n",
        "\n",
        "plt.bar(counts_entity.keys(), counts_entity.values(), color=\"#3F5D7D\", width=0.8)\n",
        "\n",
        "# Add the values to the plot\n",
        "for i, value in enumerate(counts_entity.values()):\n",
        "    plt.text(i, value, str(value), ha='center', va='bottom')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a huge unbalance between the 'O' class and the other classes."
      ],
      "metadata": {
        "id": "SlCpoLpWClDb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPw6pOW-htao"
      },
      "outputs": [],
      "source": [
        "counts_word = Counter(data_df['Word'])\n",
        "print(counts_word)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most common words are punctuation and italian stop words."
      ],
      "metadata": {
        "id": "pL8Har7iCpet"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "In0miLe7PSdf"
      },
      "source": [
        "There are three classes: person (PER), location (LOC) and organization(ORG).\n",
        "\n",
        "The tag 'O' is used when a word is not a named entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQ38QUQfUDIl"
      },
      "outputs": [],
      "source": [
        "# Retrieve all words with label 'PER'\n",
        "persons = data_df.loc[data_df['Entity'] == 'PER', 'Word']\n",
        "persons_set = set(persons)\n",
        "print(f'There are {len(persons_set)} different words labelled as PER')\n",
        "sorted(persons_set)[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSxuJSKac9uq"
      },
      "outputs": [],
      "source": [
        "# Retrieve all words with label 'LOC'\n",
        "locations = data_df.loc[data_df['Entity'] == 'LOC', 'Word']\n",
        "locations_set = set(locations)\n",
        "print(f'There are {len(locations_set)} different words labelled as LOC')\n",
        "sorted(locations_set)[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDG0YXS1c83P"
      },
      "outputs": [],
      "source": [
        "# Retrieve all words with label 'ORG'\n",
        "organizations = data_df.loc[data_df['Entity'] == 'ORG', 'Word']\n",
        "organizations_set = set(organizations)\n",
        "print(f'There are {len(organizations_set)} different words labelled as ORG')\n",
        "sorted(organizations_set)[:10]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the text file degasperi_train.txt\n",
        "file_path = os.path.join(os.getcwd(), 'dataset/txt-version/degasperi_train.txt')\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(f'The length of the text is {len(text)} characters')"
      ],
      "metadata": {
        "id": "l4FYdvAF_g6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-LgxClUkHu6"
      },
      "outputs": [],
      "source": [
        "# Obtain all the sentences\n",
        "sentences = re.split('\\n', text)\n",
        "\n",
        "print(f'There are {len(sentences)} sentences')\n",
        "sentences[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieve some statistics for all the four training dataset plus the merged one"
      ],
      "metadata": {
        "id": "c1nELGMnCO6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the list of files in the dataset directory\n",
        "dataset_dir = os.path.join(os.getcwd(), 'dataset')\n",
        "dataset_files = os.listdir(dataset_dir)\n",
        "datasets = dict()\n",
        "\n",
        "# Create a dictionary with all train datasets\n",
        "for file in tqdm(dataset_files):\n",
        "    # We are dealing with the train.tsv files only\n",
        "    if file.endswith('test.tsv') or not file.endswith('.tsv'):\n",
        "        continue\n",
        "\n",
        "    file_path = os.path.join(dataset_dir, file)  \n",
        "\n",
        "    # Inserting the content of the tsv file into a dataframe and add it to the dictionary\n",
        "    datasets[str(file)[:-4]] = pd.read_csv(file_path, sep='\\t', names=['Word', 'Entity'])"
      ],
      "metadata": {
        "id": "ufbXLgPNCwuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for dataset in datasets.keys():\n",
        "  print(f'\\nDataset {dataset}')\n",
        "  datasets[dataset].info()"
      ],
      "metadata": {
        "id": "kiCFgcfzLzxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for dataset in datasets.keys():\n",
        "  print(f'\\nDataset {dataset}')\n",
        "  display(datasets[dataset].describe())"
      ],
      "metadata": {
        "id": "4KHf94svMcLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for dataset in datasets.keys():\n",
        "  counts_entity = Counter(datasets[dataset]['Entity'])\n",
        "\n",
        "  plt.bar(counts_entity.keys(), counts_entity.values(), color=\"#3F5D7D\", width=0.8)\n",
        "\n",
        "  # Add the values to the plot\n",
        "  for i, value in enumerate(counts_entity.values()):\n",
        "      plt.text(i, value, str(value), ha='center', va='bottom')\n",
        "  plt.title(dataset)    \n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "AB2bQXtvNl_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for dataset in datasets.keys():\n",
        "  counts_word = Counter(datasets[dataset]['Word'])\n",
        "  print(f'\\nDataset {dataset} {counts_word.most_common(10)}')"
      ],
      "metadata": {
        "id": "PLl5e8zIRCdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the list of files in the dataset directory\n",
        "dataset_dir = os.path.join(os.getcwd(), 'dataset/txt-version')\n",
        "dataset_files = os.listdir(dataset_dir)\n",
        "datasets = dict()\n",
        "\n",
        "# Create a dictionary with all train datasets\n",
        "for file in tqdm(dataset_files):\n",
        "    # We are dealing with the train.txt files only\n",
        "    if file.endswith('test.txt') or not file.endswith('.txt'):\n",
        "        continue\n",
        "\n",
        "    file_path = os.path.join(dataset_dir, file)  \n",
        "\n",
        "    # Add each txt file to the dictionary\n",
        "    with open(file_path, 'r') as f:\n",
        "      datasets[str(file)[:-4]] = f.read()"
      ],
      "metadata": {
        "id": "O5Qk2WHqEurQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for dataset in datasets.keys():\n",
        "  print(f'The length of the text of {dataset} is {len(datasets[dataset])} characters')"
      ],
      "metadata": {
        "id": "XpWBOne0ULNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for dataset in datasets.keys():\n",
        "  # Obtain all the sentences\n",
        "  sentences = re.split('\\n', datasets[dataset])\n",
        "\n",
        "  print(f'The dataset {dataset} has {len(sentences)} sentences') "
      ],
      "metadata": {
        "id": "aAN5WlrJUIjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3C6GbBWsSVL"
      },
      "source": [
        "## Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Vocabulary for a single dataset, we take `degasperi_train.tsv`"
      ],
      "metadata": {
        "id": "MMKlZvh5dSPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the text file degasperi_train.txt\n",
        "file_path = os.path.join(os.getcwd(), 'dataset/txt-version/degasperi_train.txt')\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(text[:150])"
      ],
      "metadata": {
        "id": "1GbWd2eRXEvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJjoUJZ6Of6D"
      },
      "outputs": [],
      "source": [
        "regex = '[' + string.punctuation + ']'\n",
        "\n",
        "# Remove all the punctuation (maybe not necessary)\n",
        "text_no_punctuation = re.sub(regex,'',text)\n",
        "\n",
        "print(text_no_punctuation[:150])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain all the sentences\n",
        "sentences = re.split('\\n', text_no_punctuation)"
      ],
      "metadata": {
        "id": "6Vf-pZSbe43U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYcWywAJQ0Ze"
      },
      "outputs": [],
      "source": [
        "print(f'The length of the text with punctuation is {len(text)} characters')\n",
        "print(f'The length of the text without punctuation is {len(text_no_punctuation)} characters')\n",
        "print(f'{len(text) - len(text_no_punctuation)} characters are removed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WDFeXlkRTiC"
      },
      "outputs": [],
      "source": [
        "# Build vocabulary\n",
        "# Convert to lowercase, split on whitespace, select only distinct words, sort the words alphabetically\n",
        "words = text_no_punctuation.lower().split()\n",
        "vocabulary = sorted(set(words))\n",
        "print(f'The vocabulary contains {len(vocabulary)} words')\n",
        "#print(vocabulary)\n",
        "\n",
        "counts_word = Counter(words)\n",
        "print(f'Most common words: {counts_word.most_common(10)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKEP5IP8erxH"
      },
      "outputs": [],
      "source": [
        "# Build vocabulary using CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(sentences)\n",
        "\n",
        "print(f'The vocabulary contains {len(vectorizer.get_feature_names_out())} words')\n",
        "vectorizer.get_feature_names_out()[:150]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqskmFUsfE2m"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "#print('Italian stopwords:')\n",
        "#print(stopwords.words('italian'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wSNmxFQiUNQ"
      },
      "outputs": [],
      "source": [
        "# Build vocabulary by removing italian stop words and with words with at least 3 occurences\n",
        "vectorizer = CountVectorizer(min_df=3, stop_words=stopwords.words('italian'))\n",
        "vectorizer.fit(sentences)\n",
        "print(f\"vocabulary size: {len(vectorizer.get_feature_names_out())}\")\n",
        "# vectorizer.get_feature_names_out()[:50]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build vocabulary for each training set\n"
      ],
      "metadata": {
        "id": "CvUUX6Q1djq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the list of files in the dataset directory\n",
        "dataset_dir = os.path.join(os.getcwd(), 'dataset/txt-version')\n",
        "dataset_files = os.listdir(dataset_dir)\n",
        "datasets = dict()\n",
        "\n",
        "# Create a dictionary with all train datasets\n",
        "for file in tqdm(dataset_files):\n",
        "    # We are dealing with the train.txt files only\n",
        "    if file.endswith('test.txt') or not file.endswith('.txt'):\n",
        "        continue\n",
        "\n",
        "    file_path = os.path.join(dataset_dir, file)  \n",
        "\n",
        "    # Add each txt file to the dictionary\n",
        "    with open(file_path, 'r') as f:\n",
        "      datasets[str(file)[:-4]] = f.read()"
      ],
      "metadata": {
        "id": "MsIuxxxhdE9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary that contains the vocabulary of each dataset\n",
        "vocabularies = dict()\n",
        "\n",
        "regex = '[' + string.punctuation + ']'\n",
        "\n",
        "for dataset in datasets.keys():\n",
        "  # Get the text\n",
        "  text = datasets[dataset]\n",
        "\n",
        "  # Remove all the punctuation (maybe not necessary)\n",
        "  text_no_punctuation = re.sub(regex,'',text)\n",
        "\n",
        "  # Obtain all the sentences\n",
        "  sentences = re.split('\\n', text_no_punctuation)\n",
        "\n",
        "  # Build vocabulary by removing italian stop words and with words with at least 3 occurences\n",
        "  vectorizer = CountVectorizer(min_df=3, stop_words=stopwords.words('italian'))\n",
        "  vectorizer.fit(sentences)\n",
        "\n",
        "  # Save vocabulary\n",
        "  vocabularies[dataset] = vectorizer\n",
        "  print(f\"{dataset} vocabulary size: {len(vectorizer.get_feature_names_out())}\")"
      ],
      "metadata": {
        "id": "9T8Ty60xeLzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabularies['merged_dataset_train'].get_feature_names_out()[-50:]"
      ],
      "metadata": {
        "id": "gKOnHk6WjkXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjF7uzE4sYac"
      },
      "source": [
        "## Word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQxqEsSruUE8"
      },
      "source": [
        "Clean the data: \n",
        "- remove non-letter characters from each sentence \n",
        "- lowercase \n",
        "- tokenize the sentences based on whitespace\n",
        "- remove any sentence with length less than 2 since it won't be useful for training Word2Vec. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IjxHR3bsVbB"
      },
      "outputs": [],
      "source": [
        "# maybe don't remove punctuation\n",
        "tokenized_sentences = [re.sub('\\W', ' ', sentence).lower().split() for sentence in sentences]\n",
        "# remove sentences that are only 1 word long\n",
        "tokenized_sentences = [sentence for sentence in tokenized_sentences if len(sentence) > 1]\n",
        "\n",
        "for sentence in tokenized_sentences[:10]:\n",
        "    print(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OeOCEubuycD"
      },
      "source": [
        "Train Word2Vec.\n",
        "\n",
        "Parameters:\n",
        "- embedding size = 30,\n",
        "- minimum count for any vocabulary term = 1\n",
        "- size of the context window = 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ezv6PFp3ujfY"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = Word2Vec(tokenized_sentences, vector_size=30, min_count=1, window=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7D9MH_pvIuX"
      },
      "outputs": [],
      "source": [
        "print(f'There are {len(model.wv)} word embeddings')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRnDMjN0TWLT"
      },
      "outputs": [],
      "source": [
        "term = 'italia'\n",
        "model.wv[term]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUl_BzPAU_lZ"
      },
      "outputs": [],
      "source": [
        "term ='italia'\n",
        "\n",
        "model.wv.most_similar(term)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSGjDHL9YNdb"
      },
      "outputs": [],
      "source": [
        "# properties are not valid, probably because the dataset is too small\n",
        "vec = model.wv['roma'] + (model.wv['francia'] - model.wv['italia'])  \n",
        "\n",
        "model.wv.similar_by_vector(vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtUoQ1YFWp8B"
      },
      "outputs": [],
      "source": [
        "# sample 500 random word embeddings\n",
        "sample = random.sample(list(model.wv.key_to_index), 300)\n",
        "word_vectors = model.wv[sample]\n",
        "\n",
        "# visualize word embeddings using TSNE\n",
        "tsne = TSNE(n_components=3, n_iter=2000)\n",
        "tsne_embedding = tsne.fit_transform(word_vectors)\n",
        "x, y, z = np.transpose(tsne_embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFSMLtoTXsG1"
      },
      "outputs": [],
      "source": [
        "!pip install plotly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSSUvl9QW_cz"
      },
      "outputs": [],
      "source": [
        "\n",
        "fig = px.scatter_3d(x=x[:200],y=y[:200],z=z[:200],text=sample[:200])\n",
        "fig.update_traces(marker=dict(size=3,line=dict(width=2)),textfont_size=10)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRlf5AsVNhy9"
      },
      "source": [
        "## spaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dQ2OEhn7B7i"
      },
      "outputs": [],
      "source": [
        "!pip install -U spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTtyxjoxn6E4"
      },
      "source": [
        "There is also the large version https://spacy.io/models/it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88qfU6lA7Er7"
      },
      "outputs": [],
      "source": [
        "!{sys.executable} -m spacy download it_core_news_sm;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVGJtdVV7N1w"
      },
      "outputs": [],
      "source": [
        "nlp_model = it_core_news_sm.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSx_OVao7hA-"
      },
      "outputs": [],
      "source": [
        "text_spacy = text[:151]\n",
        "parsed_text = nlp_model(text_spacy)\n",
        "print(parsed_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hwh_bZCz8NPd"
      },
      "outputs": [],
      "source": [
        "print(f'The length of the original text is {len(text_spacy)} characters')\n",
        "print(f'The length of the parsed text is {len(parsed_text)} words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbgptWLF8bub"
      },
      "outputs": [],
      "source": [
        "parsed_text.ents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnPqhzHJ8geW"
      },
      "outputs": [],
      "source": [
        "print([(ent.text, ent.label_) for ent in parsed_text.ents])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ID8cgcKa8mbk"
      },
      "outputs": [],
      "source": [
        "[(X, X.ent_iob_, X.ent_type_) for X in parsed_text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngqXB3-68q0T"
      },
      "outputs": [],
      "source": [
        "from spacy import displacy\n",
        "displacy.render(parsed_text, jupyter=True, style='ent')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "JRlf5AsVNhy9"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}